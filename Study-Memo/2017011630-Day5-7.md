# 深度学习基础
## week5
> 激活函数、CNN、RNN网络结构、numpy库

## week6
> Tensorflow

这个playground好：http://playground.tensorflow.org/
![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf7fn5priyj317b0u0duo.jpg)

（有点类似）这个deeplearning.ai的讲权重初始化和优化方式的也好：https://www.deeplearning.ai/ai-notes/optimization/
![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf7fo3gc8tj31pq0u0qe1.jpg)

## Week7 AlexNet

#### 记录两张图

张量的展示：

<img src="https://tva1.sinaimg.cn/large/00831rSTly1gdjzdhqcsoj30me0g80yx.jpg" style="zoom:50%;" />

符号微分：

<img src="https://tva1.sinaimg.cn/large/00831rSTly1gdjzew9q7gj30ho0hewjp.jpg" style="zoom:50%;" />

#### AlexNet

<img src="https://tva1.sinaimg.cn/large/00831rSTly1gdjzgh75g4j317m0eidjz.jpg" style="zoom:50%;" />

* **模型结构：**
  * ReLU促进快速收敛，多GPU训练，Normalization和Overlapping Pooling降低错误率
* **防止过拟合：**
  * 数据增强，Dropout
* **结果：**
  * top1和top5错误率降低了10%
  * 减一层CNN表现下降2%: 深度的重要性

评价：在现在看Alexnet的网络设计和训练时用的各种trick相当平淡了，利用现在成熟的深度学习架构复现这篇论文中的种种实验也不是什么困难的事情。AlexNet使用的各种trick似乎也大多不是首次提出，但是这哥们自己写CUDA代码，在当时的GTX580上把各种炼丹技巧综合实验，最终做出10%的大幅度效果提高，实际展示了"深"的能力促进深度学习这波浪潮的兴起，这项工作还是非常值得敬佩的。











